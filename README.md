# Sarcasm Detection Research - Main Project

This repository contains three different approaches to sarcasm detection for thesis research.

## ğŸ“ Directory Structure

```
Server/
â”œâ”€â”€ IDL-RoBERTa/                      # Enhanced RoBERTa with incongruity features
â”‚   â”œâ”€â”€ idl_roberta_sarcasm.ipynb     # Notebook version
â”‚   â”œâ”€â”€ idl_roberta_sarcasm.py        # Script version
â”‚   â”œâ”€â”€ requirements.txt              # Dependencies
â”‚   â”œâ”€â”€ setup_environment.sh          # Setup script
â”‚   â””â”€â”€ README.md                     # Detailed documentation
â”‚
â”œâ”€â”€ RoBERTa-Baseline/                 # Standard RoBERTa baseline
â”‚   â”œâ”€â”€ roberta_baseline_sarcasm.ipynb # Notebook version
â”‚   â”œâ”€â”€ roberta_baseline_sarcasm.py    # Script version
â”‚   â”œâ”€â”€ requirements.txt               # Dependencies
â”‚   â””â”€â”€ README.md                      # Detailed documentation
â”‚
â”œâ”€â”€ RoBERTa-BackTranslation/          # RoBERTa with data augmentation
â”‚   â”œâ”€â”€ roberta_backtranslation_sarcasm.py # Training script
â”‚   â”œâ”€â”€ preprocessed_data.csv          # Augmented dataset
â”‚   â”œâ”€â”€ requirements.txt               # Dependencies
â”‚   â”œâ”€â”€ install.sh                     # CPU setup
â”‚   â”œâ”€â”€ install_gpu.sh                 # GPU setup
â”‚   â””â”€â”€ README.md                      # Detailed documentation
â”‚
â”œâ”€â”€ datasets/                          # Centralized datasets
â”‚   â”œâ”€â”€ Sarcasm_Headlines_Dataset_v2.json
â”‚   â”œâ”€â”€ Sarcasm_Headlines_Dataset.json
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ docs/                              # Documentation
    â”œâ”€â”€ IDL-RoBERTa-README.md
    â”œâ”€â”€ RoBERTa-BackTranslation-README.md
    â”œâ”€â”€ GPU_SETUP.md
    â””â”€â”€ QUICK_START.sh
```

## ğŸ¯ Research Objectives

Compare three approaches to sarcasm detection:

### 1. **RoBERTa Baseline** (Standard)
- Pure RoBERTa-base model
- No additional features
- Establishes performance baseline

### 2. **IDL-RoBERTa** (Enhanced)
- Commonsense incongruity detection
- Token-level attention mechanism
- VADER sentiment integration
- Custom architecture

### 3. **RoBERTa-BackTranslation** (Augmented)
- Data augmentation via back-translation
- Standard RoBERTa architecture
- Enhanced training data diversity

## ğŸš€ Quick Start

### Choose Your Approach:

**For Baseline Comparison:**
```bash
cd RoBERTa-Baseline
python3 roberta_baseline_sarcasm.py
```

**For Enhanced Model:**
```bash
cd IDL-RoBERTa
python3 idl_roberta_sarcasm.py
```

**For Augmented Data:**
```bash
cd RoBERTa-BackTranslation
python3 roberta_backtranslation_sarcasm.py
```

### Using Notebooks (Kaggle/Colab):
Each directory contains `.ipynb` files ready for cloud platforms.

## ğŸ“Š Dataset

**Sarcasm Headlines Dataset v2**
- **Source**: News headlines
- **Sarcastic**: The Onion
- **Non-sarcastic**: HuffPost  
- **Total Samples**: ~28,620
- **Format**: JSON (line-delimited)
- **Location**: `datasets/Sarcasm_Headlines_Dataset_v2.json`

## ğŸ”§ Requirements

All approaches require:
- Python 3.8+
- PyTorch 2.0+
- Transformers 4.30+
- Accelerate 0.26.0+
- GPU recommended (but works on CPU)

See individual `requirements.txt` in each directory.

## ğŸ“ˆ Expected Results

Each model produces:
- **Metrics**: Accuracy, F1, Precision, Recall
- **Reports**: Classification reports
- **Predictions**: CSV with true/predicted labels
- **Checkpoints**: Saved model weights

## ğŸ“ Thesis Context

This implementation compares different architectural and data augmentation strategies for sarcasm detection, measuring:
1. Impact of incongruity features (IDL-RoBERTa vs Baseline)
2. Effect of data augmentation (BackTranslation vs Baseline)
3. Trade-offs between complexity and performance

## ğŸ“ Citation

If you use this code for research, please cite appropriately.

## ğŸ¤ Contributing

This is thesis research code. For questions or collaboration, please contact the repository owner.

## ğŸ“„ License

See repository license file.

---

**Author**: Thesis Research Project  
**Date**: November 2025  
**Framework**: PyTorch + Hugging Face Transformers
