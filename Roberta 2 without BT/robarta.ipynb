{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b9740e",
   "metadata": {},
   "source": [
    "# IDL-RoBERTa for Sarcasm Detection\n",
    "## Enhanced Model with Commonsense Incongruity Features\n",
    "\n",
    "This notebook implements an Incongruity-Driven Learning (IDL) approach combined with RoBERTa for sarcasm detection using the **Sarcasm Headlines Dataset v2**.\n",
    "\n",
    "**Dataset**: News headlines from The Onion (sarcastic) and HuffPost (non-sarcastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de89fd0",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip uninstall -y transformers accelerate\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install accelerate\n",
    "!pip install protobuf==4.25.3 --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7424e09",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b06327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaPreTrainedModel, RobertaModel, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from collections import defaultdict\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf53af",
   "metadata": {},
   "source": [
    "## 3. Download NLTK Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887dda1",
   "metadata": {},
   "source": [
    "## 4. Load Sarcasm Headlines Dataset\n",
    "\n",
    "Load the JSON dataset and split into train/test sets with 80/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6105ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"üìÇ Loading Sarcasm Headlines Dataset v2...\")\n",
    "\n",
    "# Load JSON dataset (line-by-line format)\n",
    "dataset_path = '/kaggle/input/sarcasm-headlines-dataset/Sarcasm_Headlines_Dataset_v2.json'\n",
    "\n",
    "data_list = []\n",
    "with open(dataset_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} samples\")\n",
    "print(f\"üìä Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nüìä Class distribution:\")\n",
    "print(df['is_sarcastic'].value_counts())\n",
    "print(f\"\\nüìù Sample headlines:\")\n",
    "print(df[['headline', 'is_sarcastic']].head(10))\n",
    "\n",
    "# Train-test split (80/20)\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['is_sarcastic']\n",
    ")\n",
    "\n",
    "# Reset indices\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Train samples: {len(train_df)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_df)}\")\n",
    "print(f\"Train distribution: {train_df['is_sarcastic'].value_counts().to_dict()}\")\n",
    "print(f\"Test distribution: {test_df['is_sarcastic'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664081d",
   "metadata": {},
   "source": [
    "## 5. Build Enhanced Commonsense Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî® Building enhanced commonsense dictionary...\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Collect sentiment scores per word and label\n",
    "word_sentiments = defaultdict(lambda: {'non_sarc': [], 'sarc': []})\n",
    "\n",
    "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Analyzing training data\"):\n",
    "    text = row['headline']  # Changed from 'Tweet' to 'headline'\n",
    "    label = row['is_sarcastic']\n",
    "    \n",
    "    # Get sentence-level sentiment\n",
    "    sent_score = sia.polarity_scores(text)['compound']\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.isalpha() and len(token) > 2:  # Filter meaningful words\n",
    "            if label == 0:\n",
    "                word_sentiments[token]['non_sarc'].append(sent_score)\n",
    "            else:\n",
    "                word_sentiments[token]['sarc'].append(sent_score)\n",
    "\n",
    "# Build incongruity dictionary with stronger signals\n",
    "commonsense_dict = {}\n",
    "min_count = 10  # Increased for reliability\n",
    "threshold = 0.15  # Higher threshold for clearer signals\n",
    "\n",
    "for word, sents in word_sentiments.items():\n",
    "    non_sarc = sents['non_sarc']\n",
    "    sarc = sents['sarc']\n",
    "    \n",
    "    if len(non_sarc) >= min_count and len(sarc) >= min_count:\n",
    "        expected_sent = np.mean(non_sarc)\n",
    "        sarcastic_sent = np.mean(sarc)\n",
    "        incongruity = abs(expected_sent - sarcastic_sent)\n",
    "        \n",
    "        if incongruity > threshold:\n",
    "            commonsense_dict[word] = {\n",
    "                'expected': expected_sent,\n",
    "                'sarcastic': sarcastic_sent,\n",
    "                'incongruity': incongruity,\n",
    "                'flip': 1 if (expected_sent > 0 and sarcastic_sent < 0) or \n",
    "                             (expected_sent < 0 and sarcastic_sent > 0) else 0\n",
    "            }\n",
    "\n",
    "print(f\"‚úÖ Built dictionary with {len(commonsense_dict)} incongruent words\")\n",
    "\n",
    "# Show some examples\n",
    "if len(commonsense_dict) > 0:\n",
    "    print(\"\\nüìù Sample incongruent words:\")\n",
    "    sample_words = list(commonsense_dict.items())[:10]\n",
    "    for word, info in sample_words:\n",
    "        print(f\"  '{word}': incongruity={info['incongruity']:.3f}, flip={info['flip']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54ba6b",
   "metadata": {},
   "source": [
    "## 6. Define Incongruity Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a87a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_incongruity_features(text, max_len=32):\n",
    "    \"\"\"Extract token-level AND sentence-level incongruity features\"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Token-level features\n",
    "    token_features = []\n",
    "    incongruity_scores = []\n",
    "    flip_count = 0\n",
    "    \n",
    "    for token in tokens[:max_len]:\n",
    "        if token in commonsense_dict:\n",
    "            info = commonsense_dict[token]\n",
    "            incongruity = info['incongruity']\n",
    "            flip = info['flip']\n",
    "            expected = info['expected']\n",
    "            sarcastic = info['sarcastic']\n",
    "            \n",
    "            incongruity_scores.append(incongruity)\n",
    "            flip_count += flip\n",
    "            \n",
    "            # Multi-dimensional features per token\n",
    "            token_features.append([\n",
    "                incongruity,           # Magnitude of incongruity\n",
    "                flip,                  # Sentiment flip indicator\n",
    "                expected,              # Expected sentiment\n",
    "                sarcastic,             # Sarcastic sentiment\n",
    "                expected - sarcastic   # Signed difference\n",
    "            ])\n",
    "        else:\n",
    "            token_features.append([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    \n",
    "    # Pad or truncate\n",
    "    while len(token_features) < max_len:\n",
    "        token_features.append([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    token_features = token_features[:max_len]\n",
    "    \n",
    "    # Sentence-level aggregated features\n",
    "    if incongruity_scores:\n",
    "        mean_incong = np.mean(incongruity_scores)\n",
    "        max_incong = np.max(incongruity_scores)\n",
    "        std_incong = np.std(incongruity_scores)\n",
    "        flip_ratio = flip_count / len(tokens)\n",
    "    else:\n",
    "        mean_incong = max_incong = std_incong = flip_ratio = 0.0\n",
    "    \n",
    "    sentence_features = [mean_incong, max_incong, std_incong, flip_ratio]\n",
    "    \n",
    "    return (torch.tensor(token_features, dtype=torch.float), \n",
    "            torch.tensor(sentence_features, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80d480",
   "metadata": {},
   "source": [
    "## 7. Define IDL-RoBERTa Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d087aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDLRobertaForSarcasm(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.roberta = RobertaModel(config)\n",
    "        \n",
    "        self.token_feature_dim = 5  # 5 features per token\n",
    "        self.sentence_feature_dim = 4  # 4 sentence-level features\n",
    "        \n",
    "        # Token-level attention with incongruity\n",
    "        self.token_attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size + self.token_feature_dim, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Incongruity feature processor\n",
    "        self.feature_processor = nn.Sequential(\n",
    "            nn.Linear(self.sentence_feature_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        # Final classifier combining everything\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size + 32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_features, sentence_features, \n",
    "                labels=None):\n",
    "        # Get RoBERTa embeddings\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]  # [batch, seq_len, hidden]\n",
    "        \n",
    "        # Combine RoBERTa embeddings with token-level incongruity features\n",
    "        combined = torch.cat([sequence_output, token_features], dim=-1)\n",
    "        \n",
    "        # Compute attention scores using incongruity-aware features\n",
    "        attention_scores = self.token_attention(combined).squeeze(-1)\n",
    "        attention_scores = attention_scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Weighted context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), sequence_output).squeeze(1)\n",
    "        context = self.dropout(context)\n",
    "        \n",
    "        # Process sentence-level incongruity features\n",
    "        sentence_repr = self.feature_processor(sentence_features)\n",
    "        \n",
    "        # Combine context and incongruity features\n",
    "        final_repr = torch.cat([context, sentence_repr], dim=-1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(final_repr)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits} if loss is not None else logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74087053",
   "metadata": {},
   "source": [
    "## 8. Create Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=32):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['headline']  # Changed from 'Tweet' to 'headline'\n",
    "        label = item['is_sarcastic']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Extract incongruity features\n",
    "        token_feats, sentence_feats = extract_incongruity_features(text, self.max_len)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'token_features': token_feats,\n",
    "            'sentence_features': sentence_feats,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9028b",
   "metadata": {},
   "source": [
    "## 9. Prepare Training and Evaluation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd43040",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Preparing datasets...\")\n",
    "train_data = train_df.to_dict('records')\n",
    "eval_data = test_df.to_dict('records')\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_dataset = SarcasmDataset(train_data, tokenizer)\n",
    "eval_dataset = SarcasmDataset(eval_data, tokenizer)\n",
    "\n",
    "print(f\"‚úÖ Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úÖ Eval dataset: {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d2e8e",
   "metadata": {},
   "source": [
    "## 10. Define Metrics Computation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c368f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    if isinstance(labels, tuple):\n",
    "        labels = labels[0]\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    \n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df1c7d",
   "metadata": {},
   "source": [
    "## 11. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/idl_roberta_results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/kaggle/working/logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    learning_rate=2e-5,\n",
    "    report_to='none',\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,  # Effective batch size: 32\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354719d6",
   "metadata": {},
   "source": [
    "## 12. Initialize Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Initializing IDL-RoBERTa model...\")\n",
    "model = IDLRobertaForSarcasm.from_pretrained('roberta-base')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on {device}\")\n",
    "print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"üìä Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5deaa2a",
   "metadata": {},
   "source": [
    "## 13. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bf274",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b83287",
   "metadata": {},
   "source": [
    "## 14. Evaluate and Generate Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753489a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "results = trainer.evaluate()\n",
    "\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(true_labels, preds, \n",
    "                          target_names=['Non-Sarcastic', 'Sarcastic'],\n",
    "                          digits=4))\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(f\"üíæ Best model saved to: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06647972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results_file = '/kaggle/working/final_results.txt'\n",
    "with open(results_file, 'w') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"IDL-ROBERTA SARCASM DETECTION RESULTS\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Dataset: Sarcasm Headlines Dataset v2\\n\")\n",
    "    f.write(f\"Total samples: {len(df)}\\n\")\n",
    "    f.write(f\"Train samples: {len(train_df)}\\n\")\n",
    "    f.write(f\"Test samples: {len(test_df)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Evaluation Metrics:\\n\")\n",
    "    for key, value in results.items():\n",
    "        f.write(f\"  {key}: {value:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    f.write(\"DETAILED CLASSIFICATION REPORT\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(classification_report(true_labels, preds, \n",
    "                                 target_names=['Non-Sarcastic', 'Sarcastic'],\n",
    "                                 digits=4))\n",
    "\n",
    "print(f\"\\nüìÑ Results saved to: {results_file}\")\n",
    "\n",
    "# Display some example predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "sample_indices = np.random.choice(len(test_df), 10, replace=False)\n",
    "for idx in sample_indices:\n",
    "    headline = test_df.iloc[idx]['headline']\n",
    "    true_label = test_df.iloc[idx]['is_sarcastic']\n",
    "    pred_label = preds[idx]\n",
    "    \n",
    "    emoji = \"‚úÖ\" if true_label == pred_label else \"‚ùå\"\n",
    "    true_str = \"Sarcastic\" if true_label == 1 else \"Non-Sarcastic\"\n",
    "    pred_str = \"Sarcastic\" if pred_label == 1 else \"Non-Sarcastic\"\n",
    "    \n",
    "    print(f\"\\n{emoji} Headline: {headline}\")\n",
    "    print(f\"   True: {true_str} | Predicted: {pred_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aab830",
   "metadata": {},
   "source": [
    "## 15. Save Results to File"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
