{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6ee90a",
   "metadata": {},
   "source": [
    "# RoBERTa Baseline for Sarcasm Detection\n",
    "## Standard RoBERTa Model (No Incongruity Features)\n",
    "\n",
    "This notebook implements a **baseline RoBERTa model** for sarcasm detection on the Sarcasm Headlines Dataset v2.\n",
    "\n",
    "**Purpose**: Compare performance with the enhanced IDL-RoBERTa model\n",
    "\n",
    "**Dataset**: News headlines from The Onion (sarcastic) and HuffPost (non-sarcastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51042e79",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7452476",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade accelerate>=0.26.0\n",
    "!pip install -q --upgrade transformers>=4.30.0\n",
    "!pip install -q torch pandas numpy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f3aee",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    RobertaForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e8e983",
   "metadata": {},
   "source": [
    "## 3. Load Sarcasm Headlines Dataset\n",
    "\n",
    "Load the JSON dataset and perform train/test split (80/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6cb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Loading Sarcasm Headlines Dataset v2...\")\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = '/kaggle/input/sarcasm-headlines-dataset/Sarcasm_Headlines_Dataset_v2.json'\n",
    "\n",
    "# Load JSON dataset (line-by-line format)\n",
    "data_list = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            data_list.append(json.loads(line))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "df = df.dropna(subset=['headline', 'is_sarcastic'])\n",
    "df['headline'] = df['headline'].astype(str).str.strip()\n",
    "df = df[df['headline'].str.len() > 0]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} samples\")\n",
    "print(f\"\\nüìä Class distribution:\")\n",
    "print(df['is_sarcastic'].value_counts())\n",
    "print(f\"\\nüìù Sample headlines:\")\n",
    "display(df[['headline', 'is_sarcastic']].head(10))\n",
    "\n",
    "# Train-test split (80/20)\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['is_sarcastic']\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Train samples: {len(train_df)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_df)}\")\n",
    "print(f\"Train distribution: {train_df['is_sarcastic'].value_counts().to_dict()}\")\n",
    "print(f\"Test distribution: {test_df['is_sarcastic'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31669292",
   "metadata": {},
   "source": [
    "## 4. Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarcasmDataset(Dataset):\n",
    "    \"\"\"Simple dataset for RoBERTa baseline\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['headline']\n",
    "        label = item['is_sarcastic']\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a26ad3",
   "metadata": {},
   "source": [
    "## 5. Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Preparing datasets...\")\n",
    "\n",
    "# Convert to dict\n",
    "train_data = train_df.to_dict('records')\n",
    "eval_data = test_df.to_dict('records')\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"üì• Loading RoBERTa tokenizer...\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SarcasmDataset(train_data, tokenizer, max_len=128)\n",
    "eval_dataset = SarcasmDataset(eval_data, tokenizer, max_len=128)\n",
    "\n",
    "print(f\"‚úÖ Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úÖ Eval dataset: {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6494e8",
   "metadata": {},
   "source": [
    "## 6. Define Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute accuracy and F1 score\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Metrics function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4e6d2",
   "metadata": {},
   "source": [
    "## 7. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a47ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è  Configuring training arguments...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/roberta_baseline_results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/kaggle/working/logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    learning_rate=2e-5,\n",
    "    report_to='none',\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training configuration:\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   FP16: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d34408",
   "metadata": {},
   "source": [
    "## 8. Load RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffa5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Loading RoBERTa model...\")\n",
    "\n",
    "# Load pre-trained RoBERTa for sequence classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=2,  # Binary classification\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on {device}\")\n",
    "print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"üìä Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a93257",
   "metadata": {},
   "source": [
    "## 9. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Initializing Trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca64ad",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765adb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"üíæ Model saved to: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2d6b2",
   "metadata": {},
   "source": [
    "## 11. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìä Evaluation Metrics:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    preds, \n",
    "    target_names=['Non-Sarcastic', 'Sarcastic'],\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e1c95c",
   "metadata": {},
   "source": [
    "## 12. Save Results and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84944047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "results_file = '/kaggle/working/roberta_baseline_results.txt'\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"ROBERTA BASELINE - SARCASM DETECTION RESULTS\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(f\"Model: RoBERTa-base (Standard)\\n\")\n",
    "    f.write(f\"Dataset: Sarcasm Headlines Dataset v2\\n\")\n",
    "    f.write(f\"Total samples: {len(df)}\\n\")\n",
    "    f.write(f\"Train samples: {len(train_df)}\\n\")\n",
    "    f.write(f\"Test samples: {len(test_df)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Evaluation Metrics:\\n\")\n",
    "    for key, value in results.items():\n",
    "        f.write(f\"  {key}: {value:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    f.write(\"CLASSIFICATION REPORT\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(classification_report(\n",
    "        true_labels, \n",
    "        preds, \n",
    "        target_names=['Non-Sarcastic', 'Sarcastic'],\n",
    "        digits=4\n",
    "    ))\n",
    "\n",
    "print(f\"üìÑ Results saved to: {results_file}\")\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df = pd.DataFrame({\n",
    "    'headline': test_df['headline'].values,\n",
    "    'true_label': true_labels,\n",
    "    'predicted_label': preds,\n",
    "    'correct': (true_labels == preds).astype(int)\n",
    "})\n",
    "predictions_csv = '/kaggle/working/roberta_baseline_predictions.csv'\n",
    "predictions_df.to_csv(predictions_csv, index=False)\n",
    "print(f\"üìÑ Predictions saved to: {predictions_csv}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (true_labels == preds).mean()\n",
    "print(f\"\\n‚úÖ Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6910e1",
   "metadata": {},
   "source": [
    "## 13. Display Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìù SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show random sample predictions\n",
    "sample_indices = np.random.choice(len(test_df), 15, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    headline = test_df.iloc[idx]['headline']\n",
    "    true_label = true_labels[idx]\n",
    "    pred_label = preds[idx]\n",
    "    \n",
    "    emoji = \"‚úÖ\" if true_label == pred_label else \"‚ùå\"\n",
    "    true_str = \"Sarcastic\" if true_label == 1 else \"Non-Sarcastic\"\n",
    "    pred_str = \"Sarcastic\" if pred_label == 1 else \"Non-Sarcastic\"\n",
    "    \n",
    "    print(f\"\\n{emoji} {headline[:90]}...\")\n",
    "    print(f\"   True: {true_str:15} | Predicted: {pred_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ BASELINE EXPERIMENT COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22712a",
   "metadata": {},
   "source": [
    "## 14. Performance Summary\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "**RoBERTa Baseline**: Standard RoBERTa-base model\n",
    "- No additional features\n",
    "- Direct classification from text\n",
    "- Simpler architecture\n",
    "\n",
    "**IDL-RoBERTa** (compare with other notebook):\n",
    "- Enhanced with incongruity features\n",
    "- Commonsense knowledge integration\n",
    "- Token-level attention mechanism\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Compare the results from this baseline with the IDL-RoBERTa model to measure the improvement gained from incongruity features!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
